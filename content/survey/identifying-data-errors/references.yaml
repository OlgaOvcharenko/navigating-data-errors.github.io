- bibTeX: "@article{krishnan2016activeclean, 
    \ title={Activeclean: Interactive data cleaning for statistical modeling},
    \ author={Krishnan, Sanjay and Wang, Jiannan and Wu, Eugene and Franklin, Michael J and Goldberg, Ken},
    \ journal={Proceedings of the VLDB Endowment},
    \ volume={9},
    \ number={12},
    \ pages={948--959},
    \ year={2016},
    \ publisher={VLDB Endowment}
    \ }"
  title: 'ActiveClean: Interactive Data Cleaning For Statistical Modeling'
  authors:
  - S Krishnan
  - J Wang
  - E Wu
  - MJ Franklin
  - K Goldberg
  abstract: |
    Analysts often clean dirty data iteratively-cleaning some data, executing the analysis, and then cleaning more 
    data based on the results. We explore the iterative cleaning process in the context of statistical model training, 
    which is an increasingly popular form of data analytics. We propose ActiveClean, which allows for progressive and 
    iterative cleaning in statistical modeling problems while preserving convergence guarantees. ActiveClean supports 
    an important class of models called convex loss models (e.g., linear regression and SVMs), and prioritizes 
    cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets 
    UCI Adult, UCI EEG, MNIST, IMDB, and Dollars For Docs with both real and synthetic errors. The results show that 
    our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. 
    Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate 
    models than uniform sampling and Active Learning.
  year: 2016
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://www.vldb.org/pvldb/vol9/p948-krishnan.pdf
    website: https://activeclean.github.io/
  id: krishnan2016activeclean
  thumbnail: /images/papers/krishnan2016activeclean.png
  type: publication
  venueLong: Proceedings of the VLDB Endowment
  venueShort: VLDB
  venueTrack: null
  group: Quantifying Data Importance
  tag: data-importance-general

- bibTeX: "@inproceedings{koh2017understanding, 
    \ title={Understanding Black-box Predictions via Influence Functions},
    \ author={Koh, Pang Wei and Liang, Percy},
    \ booktitle={Proceedings of the 34th International Conference on Machine Learning},
    \ pages={1885--1894},
    \ year={2017},
    \ organization={PMLR}
    \ }"
  title: 'Understanding Black-box Predictions via Influence Functions'
  authors:
  - PW Koh
  - P Liang
  abstract: |
    How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic
    technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its 
    training data, thereby identifying training points most responsible for a given prediction. To scale up influence 
    functions to modern machine learning settings, we develop a simple, efficient implementation that requires only 
    oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable 
    models where the theory breaks down, approximations to influence functions can still provide valuable information. 
    On linear models and convolutional neural networks, we demonstrate that influence functions are useful for 
    multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating 
    visually-indistinguishable training-set attacks.
  year: 2017
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://proceedings.mlr.press/v70/koh17a
    code: https://github.com/kohpangwei/influence-release
  id: koh2017understanding
  thumbnail: /images/papers/koh2017understanding.png
  type: publication
  venueLong: Proceedings of the 34th International Conference on Machine Learning
  venueShort: ICML
  venueTrack: null
  group: Quantifying Data Importance
  tag: data-importance-general

- bibTeX: "@article{northcutt2021confident,
    \ title={Confident learning: Estimating uncertainty in dataset labels},
    \ author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
    \ journal={Journal of Artificial Intelligence Research},
    \ volume={70},
    \ pages={1373--1411},
    \ year={2021}
    \ }"
  title: 'Confident Learning: Estimating Uncertainty in Dataset Labels'
  authors:
  - C Northcutt
  - L Jiang
  - I Chuang
  abstract: |
    Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not 
    label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by 
    characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, 
    counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. 
    Whereas numerous studies have developed these principles independently, here, we combine them, building on the 
    assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) 
    labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and 
    experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL 
    performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. 
    Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several 
    label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon 
    Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile 
    images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for 
    ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.
  year: 2021
  entryType: article
  firstPage: 1
  links:
    paper: https://jair.org/index.php/jair/article/view/12125
    blog: https://l7.curtisnorthcutt.com/confident-learning
    code: https://github.com/cleanlab/cleanlab
  id: northcutt2021confident
  thumbnail: /images/papers/northcutt2021confident.png
  type: publication
  venueLong: Journal of Artificial Intelligence Research
  venueShort: JAIR
  venueTrack: null
  group: Quantifying Data Importance
  tag: data-importance-general

- bibTeX: "@inproceedings{ilyas2022datamodels,
    \ title={Datamodels: Predicting Predictions from Training Data},
    \ author={Ilyas, Andrew and Park, Sung Min and Engstrom, Logan and Leclerc, Guillaume and Madry, Aleksander},
    \ booktitle={Proceedings of the 39th International Conference on Machine Learning},
    \ year={2022}
    \ }"
  title: 'DataModels: Predicting Predictions from Training Data'
  authors:
  - A Ilyas
  - SM Park
  - L Engstrom
  - G Leclerc
  - A Madry
  abstract: |
    We present a conceptual framework, datamodeling, for analyzing the behavior of a model class in terms of the
    training data. For any fixed “target” example x, training set S, and learning algorithm, a datamodel is a 
    parameterized function that for any subset S' of the training set S - using only information about which
    examples of S are contained in S' - predicts the outcome of training a model on S' and evaluating on x.
    Despite the complexity of the underlying process being approximated (e.g. end-to-end training and evaluation 
    of deep neural networks), we show that even simple linear datamodels successfully predict model outputs. 
    We then demonstrate that datamodels give rise to a variety of applications, such as: accurately predicting 
    the effect of dataset counterfactuals; identifying brittle predictions; finding semantically similar examples; 
    quantifying train-test leakage; and embedding data into a well-behaved and feature-rich representation space.
  year: 2022
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://proceedings.mlr.press/v162/ilyas22a
    blog: https://gradientscience.org/datamodels-1/
    code: https://github.com/MadryLab/datamodels
  id: ilyas2022datamodels
  thumbnail: /images/papers/ilyas2022datamodels.png
  type: publication
  venueLong: Proceedings of the 39th International Conference on Machine Learning
  venueShort: ICML
  venueTrack: null
  group: Quantifying Data Importance
  tag: data-importance-general

- bibTeX: "@inproceedings{jia2021scalability,
    \ title={Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification?},
    \ author={Jia, Ruoxi and Wu, Fan and Sun, Xuehui and Xu, Jiacen and Dao, David and Kailkhura, Bhavya and Zhang, Ce and Li, Bo and Song, Dawn},
    \ booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    \ pages={8239--8247},
    \ year={2021}
    \ }"
  title: 'Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?'
  authors:
  - R Jia
  - F Wu
  - X Sun
  - J Xu
  - D Dao
  - B Kailkhura
  - C Zhang
  - B Li
  - D Song
  abstract: |
    Quantifying the importance of each training point to a learning task is a fundamental problem in machine learning 
    and the estimated importance scores have been leveraged to guide a range of data workflows such as data 
    summarization and domain adaption. One simple idea is to use the leave-one-out error of each training point to 
    indicate its importance. Recent work has also proposed to use the Shapley value, as it defines a unique value 
    distribution scheme that satisfies a set of appealing properties. However, calculating Shapley values is often 
    expensive, which limits its applicability in real-world applications at scale. Multiple heuristics to improve the 
    scalability of calculating Shapley values have been proposed recently, with the potential risk of compromising 
    their utility in real-world applications. How well do existing data quantification methods perform on existing 
    workflows? How do these methods compare with each other, empirically and theoretically? Must we sacrifice 
    scalability for the utility in these workflows when using these methods? In this paper, we conduct a novel 
    theoretical analysis comparing the utility of different importance quantification methods, and report extensive 
    experimental studies on settings such as noisy label detection, watermark removal, data summarization, data 
    acquisition, and domain adaptation on existing and proposed workflows. We show that Shapley value approximation 
    based on a KNN surrogate over pre-trained feature embeddings obtains comparable utility with existing algorithms 
    while achieving significant scalability improvement, often by orders of magnitude. Our theoretical analysis also 
    justifies its advantage over the leave-one-out error.
  year: 2021
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.html
    code: https://github.com/AI-secure/Shapley-Study
  id: jia2021scalability
  thumbnail: /images/papers/jia2021scalability.png
  type: publication
  venueLong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  venueShort: CVPR
  venueTrack: null
  group: Shapley Value as a Data Importance Metric
  tag: data-importance-shapley

- bibTeX: "@inproceedings{ghorbani2019data,
    \ title={Data shapley: Equitable valuation of data for machine learning},
    \ author={Ghorbani, Amirata and Zou, James},
    \ booktitle={International conference on machine learning},
    \ pages={2242--2251},
    \ year={2019},
    \ organization={PMLR}
    \ }"
  title: 'Data Shapley: Equitable Valuation of Data for Machine Learning'
  authors:
  - A Ghorbani
  - J Zou
  abstract: |
    As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify 
    the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it 
    has been suggested that individuals should be compensated for the data that they generate, but it is not clear 
    what is an equitable valuation for individual data. In this work, we develop a principled framework to address 
    data valuation in the context of supervised machine learning. Given a learning algorithm trained on n data points 
    to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the 
    predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. 
    We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical 
    settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition 
    to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data 
    Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in 
    providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively 
    capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to 
    improve the predictor.
  year: 2019
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://proceedings.mlr.press/v97/ghorbani19c.html
    code: https://github.com/amiratag/DataShapley
  id: ghorbani2019data
  thumbnail: /images/papers/ghorbani2019data.png
  type: publication
  venueLong: International Conference on Machine Learning
  venueShort: ICML
  venueTrack: null
  group: Shapley Value as a Data Importance Metric
  tag: data-importance-shapley

- bibTeX: "@inproceedings{kwon2022beta,
    \ title={Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning},
    \ author={Kwon, Yongchan and Zou, James},
    \ booktitle={International Conference on Artificial Intelligence and Statistics},
    \ pages={8780--8802},
    \ year={2022},
    \ organization={PMLR}
    \ }"
  title: 'Beta Shapley: A Unified and Noise-reduced Data Valuation Framework for Machine Learning'
  authors:
  - Y Kwon
  - J Zou
  abstract: |
    Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual 
    datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. 
    In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley 
    arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine 
    learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a 
    special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose 
    efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation 
    methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with 
    subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact 
    on the model.
  year: 2022
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://proceedings.mlr.press/v151/kwon22a
    code: https://github.com/ykwon0407/beta_shapley
  id: kwon2022beta
  thumbnail: /images/papers/kwon2022beta.png
  type: publication
  venueLong: International Conference on Artificial Intelligence and Statistics
  venueShort: AISTATS
  venueTrack: null
  group: Shapley Value as a Data Importance Metric
  tag: data-importance-shapley

- bibTeX: "@article{jia2019efficient,
    \ title={Efficient task-specific data valuation for nearest neighbor algorithms},
    \ author={Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Gurel, Nezihe Merve and Li, Bo and Zhang, Ce and Spanos, Costas and Song, Dawn},
    \ journal={Proceedings of the VLDB Endowment},
    \ volume={12},
    \ number={11},
    \ pages={1610--1623},
    \ year={2019},
    \ publisher={VLDB Endowment}
    \ }"
  title: 'Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms'
  authors:
  - R Jia
  - D Dao
  - B Wang
  - FA Hubis
  - NM Gurel
  - B Li
  - C Zhang
  - C Spanos
  - D Song
  abstract: |
    Given a data set D containing millions of data points and a data consumer who is willing to pay for $X 
    to train a machine learning (ML) model over D, how should we distribute this $X to each data point to 
    reflect its "value"? In this paper, we define the "relative value of data" via the Shapley value, as it 
    uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and 
    decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to 
    compute: to get Shapley values for all N data points, it requires O(2N) model evaluations for exact 
    computation and O(N log N) for (ϵ, δ)-approximation.

    In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most 
    surprising result is that for unweighted KNN classifiers and regressors, the Shapley value of all N data points 
    can be computed, exactly, in O(N log N) time - an exponential improvement on computational complexity! 
    Moreover, for (ϵ, δ)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) 
    with only sublinear complexity O(Nh(ϵ, K) log N) when ϵ is not too small and K is not too large. We empirically 
    evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of 
    magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate 
    the value calculation process even further.

    We then extend our algorithm to other scenarios such as (1) weighed KNN classifiers, (2) different data points 
    are clustered by different data curators, and (3) there are data analysts providing computation who also 
    requires proper valuation. Some of these extensions, although also being improved exponentially, are less 
    practical for exact computation (e.g., O(NK) complexity for weigthed KNN). We thus propose an Monte Carlo 
    approximation algorithm, which is O(N(log N)2/(log K)2) times more efficient than the baseline 
    approximation algorithm.
  year: 2019
  entryType: article
  firstPage: 1
  links:
    paper: https://www.vldb.org/pvldb/vol12/p1610-jia.pdf
    code: https://github.com/AI-secure/KNN-PVLDB
  id: jia2019efficient
  thumbnail: /images/papers/jia2019efficient.png
  type: publication
  venueLong: Proceedings of the VLDB Endowment
  venueShort: VLDB
  venueTrack: null
  group: Shapley Value as a Data Importance Metric
  tag: data-importance-shapley

- bibTeX: "@inproceedings{wang2024data,
    \ title={Data Shapley in One Training Run},
    \ author={Wang, Jiachen T and Mittal, Prateek and Song, Dawn and Jia, Ruoxi},
    \ booktitle={The Thirteenth International Conference on Learning Representations}
    \ year={2024}
    \ }"
  title: 'Data Shapley in One Training Run'
  authors:
  - JT Wang
  - P Mittal
  - D Song
  - R Jia
  abstract: |
    Data Shapley offers a principled framework for attributing the contribution of data within machine learning 
    contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, 
    which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition 
    cannot evaluate the contribution of data for a specific model training run, which may often be of interest in 
    practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model 
    retraining and is specifically designed for assessing data contribution for a particular model of interest. 
    In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values 
    throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data 
    Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime 
    overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform 
    data attribution for the foundation model pretraining stage. We present several case studies that offer fresh 
    insights into pretraining data's contribution and discuss their implications for copyright in generative AI 
    and pretraining data curation.
  year: 2024
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://openreview.net/forum?id=HD6bWcj87Y
    blog: https://jiachen-t-wang.github.io/data-shapley.github.io/
  id: wang2024data
  thumbnail: /images/papers/wang2024data.png
  type: publication
  venueLong: The Thirteenth International Conference on Learning Representations
  venueShort: ICLR
  venueTrack: null
  group: Shapley Value as a Data Importance Metric
  tag: data-importance-shapley

- bibTeX: "@inproceedings{pradhan2022interpretable,
    \ title={Interpretable data-based explanations for fairness debugging},
    \ author={Pradhan, Romila and Zhu, Jiongli and Glavic, Boris and Salimi, Babak},
    \ booktitle={Proceedings of the 2022 international conference on management of data},
    \ pages={247--261},
    \ year={2022}
    \ }"
  title: 'Interpretable Data-based Explanations for Fairness Debugging'
  authors:
  - R Pradhan
  - J Zhu
  - B Glavic
  - B Salimi
  abstract: |
    A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed 
    in the literature to identify bias in machine learning models that are used in critical real-life contexts. 
    However, merely reporting on a model's bias or generating explanations using existing XAI techniques is 
    insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces 
    compact, interpretable, and causal explanations for bias or unexpected model behavior by identifying coherent 
    subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept of 
    causal responsibility that quantifies the extent to which intervening on training data by removing or updating 
    subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating 
    the top-k patterns that explain model bias by utilizing techniques from the machine learning (ML) community to 
    approximate causal responsibility, and using pruning rules to manage the large search space for patterns. 
    Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations 
    for identifying and debugging sources of bias.
  year: 2022
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://dl.acm.org/doi/abs/10.1145/3514221.3517886
  id: pradhan2022interpretable
  thumbnail: /images/papers/pradhan2022interpretable.png
  type: publication
  venueLong: Proceedings of the 2022 International Conference on Management of Data
  venueShort: SIGMOD
  venueTrack: null
  group: Applying Data Importance
  tag: data-importance-application

- bibTeX: "@article{lyu2023improving,
    \ title={Improving retrieval-augmented large language models via data importance learning},
    \ author={Lyu, Xiaozhong and Grafberger, Stefan and Biegel, Samantha and Wei, Shaopeng and Cao, Meng and Schelter, Sebastian and Zhang, Ce},
    \ journal={arXiv preprint arXiv:2307.03027},
    \ year={2023}
    \ }"
  title: 'Improving Retrieval-Augmented Large Language Models via Data Importance Learning'
  authors:
  - X Lyu
  - S Grafberger
  - S Biegel
  - S Wei
  - M Cao
  - S Schelter
  - C Zhang
  abstract: |
    Retrieval augmentation enables large language models to take advantage of external knowledge, for example on 
    tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is 
    limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based 
    on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many 
    terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that 
    computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the 
    data importance of data points in the retrieval corpus using the multilinear extension of the model's utility 
    function. We further proposed an even more efficient ({\epsilon}, {\delta})-approximation algorithm. Our 
    experimental results illustrate that we can enhance the performance of large language models by only pruning or 
    reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small 
    model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). 
    Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in 
    less than ten minutes for a corpus with 100 million elements).
  year: 2023
  entryType: article
  firstPage: 1
  links:
    paper: https://arxiv.org/abs/2307.03027
    code: https://github.com/amsterdata/ragbooster
  id: lyu2023improving
  thumbnail: /images/papers/lyu2023improving.png
  type: publication
  venueLong: arXiv preprint arXiv:2307.03027
  venueShort: arXiv
  venueTrack: null
  group: Applying Data Importance
  tag: data-importance-application
